{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281fbda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc84c43",
   "metadata": {},
   "source": [
    "1. Numpy与PyTorch的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c80bea55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy数组:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "形状: (2, 2), 类型: int32\n",
      "\n",
      "PyTorch张量:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "形状: torch.Size([2, 2]), 类型: torch.int64\n",
      "设备: cpu\n"
     ]
    }
   ],
   "source": [
    "# NumPy数组\n",
    "np_array = np.array([[1, 2], [3, 4]])\n",
    "print(f\"NumPy数组:\\n{np_array}\")\n",
    "print(f\"形状: {np_array.shape}, 类型: {np_array.dtype}\")\n",
    "\n",
    "# PyTorch张量\n",
    "torch_tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "print(f\"\\nPyTorch张量:\\n{torch_tensor}\")\n",
    "print(f\"形状: {torch_tensor.shape}, 类型: {torch_tensor.dtype}\")\n",
    "print(f\"设备: {torch_tensor.device}\")  # PyTorch独有！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80fe22",
   "metadata": {},
   "source": [
    "2. 创建Tensor的12种方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b33a4d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch创建Tensor的12种方式\n",
      "==================================================\n",
      "1. torch.tensor(): \n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "2. torch.from_numpy(): \n",
      "tensor([[5, 6],\n",
      "        [7, 8]], dtype=torch.int32)\n",
      "\n",
      "3. torch.zeros(2,3): \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "4. torch.ones(2,3): \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "5. torch.eye(3): \n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "\n",
      "6. torch.empty(2,3): \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "7. torch.rand(2,3): \n",
      "tensor([[0.1140, 0.3277, 0.6931],\n",
      "        [0.3138, 0.9770, 0.8915]])\n",
      "\n",
      "8. torch.randn(2,3): \n",
      "tensor([[-0.9409,  0.4144,  0.1627],\n",
      "        [ 1.5300, -1.3503, -0.3013]])\n",
      "\n",
      "9. torch.randint(0,10,(2,3)): \n",
      "tensor([[3, 3, 0],\n",
      "        [3, 7, 3]])\n",
      "\n",
      "10. torch.arange(0,10,2): tensor([0, 2, 4, 6, 8])\n",
      "\n",
      "11. torch.linspace(0,1,5): tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
      "\n",
      "12. torch.full((2,3),3.14): \n",
      "tensor([[3.1400, 3.1400, 3.1400],\n",
      "        [3.1400, 3.1400, 3.1400]])\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch创建Tensor的12种方式\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. torch.tensor() - 从Python数据创建（最常用）\n",
    "data = [[1, 2], [3, 4]]\n",
    "t1 = torch.tensor(data)\n",
    "print(f\"1. torch.tensor(): \\n{t1}\")\n",
    "\n",
    "# 2. torch.from_numpy() - NumPy数组转换\n",
    "np_array = np.array([[5, 6], [7, 8]])\n",
    "t2 = torch.from_numpy(np_array)  # 共享内存！\n",
    "print(f\"\\n2. torch.from_numpy(): \\n{t2}\")\n",
    "\n",
    "# 3. torch.zeros() - 全0张量\n",
    "t3 = torch.zeros(2, 3)  # 2行3列\n",
    "print(f\"\\n3. torch.zeros(2,3): \\n{t3}\")\n",
    "\n",
    "# 4. torch.ones() - 全1张量\n",
    "t4 = torch.ones(2, 3)\n",
    "print(f\"\\n4. torch.ones(2,3): \\n{t4}\")\n",
    "\n",
    "# 5. torch.eye() - 单位矩阵\n",
    "t5 = torch.eye(3)  # 3×3单位矩阵\n",
    "print(f\"\\n5. torch.eye(3): \\n{t5}\")\n",
    "\n",
    "# 6. torch.empty() - 未初始化张量（注意：值是随机的！）\n",
    "t6 = torch.empty(2, 3)\n",
    "print(f\"\\n6. torch.empty(2,3): \\n{t6}\")\n",
    "\n",
    "# 7. torch.rand() - [0,1)均匀分布\n",
    "t7 = torch.rand(2, 3)\n",
    "print(f\"\\n7. torch.rand(2,3): \\n{t7}\")\n",
    "\n",
    "# 8. torch.randn() - 标准正态分布 N(0,1)\n",
    "t8 = torch.randn(2, 3)\n",
    "print(f\"\\n8. torch.randn(2,3): \\n{t8}\")\n",
    "\n",
    "# 9. torch.randint() - 整数随机数\n",
    "t9 = torch.randint(0, 10, (2, 3))  # [0,10)的整数\n",
    "print(f\"\\n9. torch.randint(0,10,(2,3)): \\n{t9}\")\n",
    "\n",
    "# 10. torch.arange() - 等差序列\n",
    "t10 = torch.arange(0, 10, 2)  # 0到10，步长2\n",
    "print(f\"\\n10. torch.arange(0,10,2): {t10}\")\n",
    "\n",
    "# 11. torch.linspace() - 等间隔序列\n",
    "t11 = torch.linspace(0, 1, 5)  # 0到1，等分5份\n",
    "print(f\"\\n11. torch.linspace(0,1,5): {t11}\")\n",
    "\n",
    "# 12. torch.full() - 填充指定值\n",
    "t12 = torch.full((2, 3), 3.14)  # 2×3矩阵，填充3.14\n",
    "print(f\"\\n12. torch.full((2,3),3.14): \\n{t12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf611daf",
   "metadata": {},
   "source": [
    "3. Tensor的核心属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fe3a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor属性检查：\n",
      "1. 形状 shape: torch.Size([2, 3, 4])\n",
      "2. 维度 dim/ndim: 3\n",
      "3. 元素总数 numel: 24\n",
      "4. 数据类型 dtype: torch.float32\n",
      "5. 存储设备 device: cpu\n",
      "6. 是否求导 requires_grad: False\n",
      "7. 梯度 grad: None\n",
      "8. 步长 stride: (12, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个示例Tensor\n",
    "x = torch.randn(2, 3, 4)  # 2个3×4矩阵\n",
    "\n",
    "print(\"Tensor属性检查：\")\n",
    "print(f\"1. 形状 shape: {x.shape}\")         # torch.Size([2, 3, 4])\n",
    "print(f\"2. 维度 dim/ndim: {x.dim()}\")      # 3维\n",
    "print(f\"3. 元素总数 numel: {x.numel()}\")   # 2×3×4=24\n",
    "print(f\"4. 数据类型 dtype: {x.dtype}\")     # torch.float32\n",
    "print(f\"5. 存储设备 device: {x.device}\")   # cpu 或 cuda:0\n",
    "print(f\"6. 是否求导 requires_grad: {x.requires_grad}\")  # False\n",
    "print(f\"7. 梯度 grad: {x.grad}\")           # None（未计算梯度）\n",
    "print(f\"8. 步长 stride: {x.stride()}\")     # (12, 4, 1) - 内存布局"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce7be0",
   "metadata": {},
   "source": [
    "4. 逐元素运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18659b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逐元素运算:\n",
      "x = tensor([1., 2., 3., 4.])\n",
      "y = tensor([5., 6., 7., 8.])\n",
      "\n",
      "加法: tensor([ 6.,  8., 10., 12.])\n",
      "减法: tensor([-4., -4., -4., -4.])\n",
      "乘法: tensor([ 5., 12., 21., 32.])\n",
      "除法: tensor([0.2000, 0.3333, 0.4286, 0.5000])\n",
      "幂运算: tensor([ 1.,  4.,  9., 16.])\n",
      "平方根: tensor([1.0000, 1.4142, 1.7321, 2.0000])\n",
      "指数: tensor([ 2.7183,  7.3891, 20.0855, 54.5981])\n",
      "对数: tensor([0.0000, 0.6931, 1.0986, 1.3863])\n",
      "\n",
      "原地加法后 x = tensor([ 6.,  8., 10., 12.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "y = torch.tensor([5.0, 6.0, 7.0, 8.0])\n",
    "\n",
    "print(\"逐元素运算:\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"\\n加法: {x + y}\")           # 或 torch.add(x, y)\n",
    "print(f\"减法: {x - y}\")           # 或 torch.sub(x, y)\n",
    "print(f\"乘法: {x * y}\")           # 或 torch.mul(x, y)\n",
    "print(f\"除法: {x / y}\")           # 或 torch.div(x, y)\n",
    "print(f\"幂运算: {x ** 2}\")        # 或 torch.pow(x, 2)\n",
    "print(f\"平方根: {torch.sqrt(x)}\")  # 要求非负\n",
    "print(f\"指数: {torch.exp(x)}\")\n",
    "print(f\"对数: {torch.log(x)}\")\n",
    "\n",
    "# 原地操作（节省内存）\n",
    "x.add_(y)  # 等价于 x = x + y，但直接修改x\n",
    "print(f\"\\n原地加法后 x = {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca6d40",
   "metadata": {},
   "source": [
    "5. 矩阵运算（深度学习核心！）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "矩阵运算演示:\n",
      "A:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "B:\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "v: tensor([2., 3.])\n",
      "\n",
      "1. 矩阵乘法:\n",
      "A @ B:\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "torch.matmul(A, B):\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "A.mm(B):\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "\n",
      "2. 矩阵-向量乘法:\n",
      "A @ v: tensor([ 8., 18.])\n",
      "A.mv(v): tensor([ 8., 18.])\n",
      "\n",
      "3. 转置:\n",
      "A.T:\n",
      "tensor([[1., 3.],\n",
      "        [2., 4.]])\n",
      "torch.transpose(A, 0, 1):\n",
      "tensor([[1., 3.],\n",
      "        [2., 4.]])\n",
      "\n",
      "4. 逆矩阵:\n",
      "A.inverse():\n",
      "tensor([[-2.0000,  1.0000],\n",
      "        [ 1.5000, -0.5000]])\n",
      "\n",
      "5. 行列式:\n",
      "torch.det(A): -2.0\n"
     ]
    }
   ],
   "source": [
    "# 创建矩阵\n",
    "A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "v = torch.tensor([2, 3], dtype=torch.float32)\n",
    "\n",
    "print(\"矩阵运算演示:\")\n",
    "print(f\"A:\\n{A}\")\n",
    "print(f\"B:\\n{B}\")\n",
    "print(f\"v: {v}\")\n",
    "\n",
    "print(\"\\n1. 矩阵乘法:\")\n",
    "print(f\"A @ B:\\n{A @ B}\")                  # Python 3.5+ 语法\n",
    "print(f\"torch.matmul(A, B):\\n{torch.matmul(A, B)}\")\n",
    "print(f\"A.mm(B):\\n{A.mm(B)}\")              # 专门用于矩阵乘法\n",
    "\n",
    "print(\"\\n2. 矩阵-向量乘法:\")\n",
    "print(f\"A @ v: {A @ v}\")\n",
    "print(f\"A.mv(v): {A.mv(v)}\")\n",
    "\n",
    "print(\"\\n3. 转置:\")\n",
    "print(f\"A.T:\\n{A.T}\")                      # 或 A.t()\n",
    "print(f\"torch.transpose(A, 0, 1):\\n{torch.transpose(A, 0, 1)}\")\n",
    "\n",
    "print(\"\\n4. 逆矩阵:\")\n",
    "if A.shape[0] == A.shape[1]:  # 方阵才有逆\n",
    "    print(f\"A.inverse():\\n{A.inverse()}\")  # 或 torch.inverse(A)\n",
    "\n",
    "print(\"\\n5. 行列式:\")\n",
    "print(f\"torch.det(A): {torch.det(A)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72949cb",
   "metadata": {},
   "source": [
    "6. 广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9576b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "广播机制演示:\n",
      "x + 1:\n",
      "tensor([[2, 3],\n",
      "        [4, 5]])\n",
      "\n",
      "a: torch.Size([1, 3]), b: torch.Size([3, 1])\n",
      "a + b:\n",
      "tensor([[5, 6, 7],\n",
      "        [6, 7, 8],\n",
      "        [7, 8, 9]])\n",
      "\n",
      "c: torch.Size([3, 4, 5]), d: torch.Size([5])\n",
      "c + d 可行: torch.Size([3, 4, 5])\n",
      "\n",
      "手动广播:\n",
      "e.unsqueeze(1): torch.Size([3, 1, 1, 4])\n",
      "f.expand(3, 5, 4): torch.Size([3, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"广播机制演示:\")\n",
    "# 情况1: 标量与Tensor\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "print(f\"x + 1:\\n{x + 1}\")  # 标量广播到整个矩阵\n",
    "\n",
    "# 情况2: 不同形状Tensor\n",
    "a = torch.tensor([[1, 2, 3]])        # 形状: (1, 3)\n",
    "b = torch.tensor([[4], [5], [6]])    # 形状: (3, 1)\n",
    "print(f\"\\na: {a.shape}, b: {b.shape}\")\n",
    "print(f\"a + b:\\n{a + b}\")            # 广播为 (3, 3)\n",
    "\n",
    "# 情况3: 自动扩展维度\n",
    "c = torch.randn(3, 4, 5)\n",
    "d = torch.randn(5)                   # 形状: (5,)\n",
    "print(f\"\\nc: {c.shape}, d: {d.shape}\")\n",
    "print(f\"c + d 可行: {(c + d).shape}\")  # d广播为(1,1,5)再扩展为(3,4,5)\n",
    "\n",
    "# 手动广播\n",
    "e = torch.randn(3, 1, 4)\n",
    "f = torch.randn(1, 5, 4)\n",
    "print(f\"\\n手动广播:\")\n",
    "print(f\"e.unsqueeze(1): {e.unsqueeze(1).shape}\")  # 在位置1增加维度\n",
    "print(f\"f.expand(3, 5, 4): {f.expand(3, 5, 4).shape}\")  # 扩展维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b292705",
   "metadata": {},
   "source": [
    "7. 统计运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9313b1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "统计运算演示:\n",
      "原始Tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "\n",
      "1. 求和:\n",
      "所有元素和: 45.0\n",
      "按列求和: tensor([12., 15., 18.])\n",
      "按行求和: tensor([ 6., 15., 24.])\n",
      "\n",
      "2. 均值:\n",
      "所有元素均值: 5.0\n",
      "按列均值: tensor([4., 5., 6.])\n",
      "\n",
      "3. 极值:\n",
      "最大值: 9.0\n",
      "最大值及其索引: torch.return_types.max(\n",
      "values=tensor([3., 6., 9.]),\n",
      "indices=tensor([2, 2, 2]))\n",
      "\n",
      "4. 其他统计量:\n",
      "标准差: 2.7386128902435303\n",
      "方差: 7.5\n",
      "中位数: 5.0\n",
      "众数: torch.return_types.mode(\n",
      "values=tensor(1.),\n",
      "indices=tensor(0))\n",
      "\n",
      "5. 累加运算:\n",
      "累积和: tensor([[ 1.,  2.,  3.],\n",
      "        [ 5.,  7.,  9.],\n",
      "        [12., 15., 18.]])\n",
      "累积积: tensor([[  1.,   2.,   6.],\n",
      "        [  4.,  20., 120.],\n",
      "        [  7.,  56., 504.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6], \n",
    "                  [7, 8, 9]], dtype=torch.float32)\n",
    "\n",
    "print(\"统计运算演示:\")\n",
    "print(f\"原始Tensor:\\n{x}\")\n",
    "\n",
    "print(\"\\n1. 求和:\")\n",
    "print(f\"所有元素和: {x.sum()}\")            # 45\n",
    "print(f\"按列求和: {x.sum(dim=0)}\")        # [12, 15, 18]\n",
    "print(f\"按行求和: {x.sum(dim=1)}\")        # [6, 15, 24]\n",
    "\n",
    "print(\"\\n2. 均值:\")\n",
    "print(f\"所有元素均值: {x.mean()}\")         # 5.0\n",
    "print(f\"按列均值: {x.mean(dim=0)}\")       # [4, 5, 6]\n",
    "\n",
    "print(\"\\n3. 极值:\")\n",
    "print(f\"最大值: {x.max()}\")                # 9\n",
    "print(f\"最大值及其索引: {x.max(dim=1)}\")   # 每行最大值和索引\n",
    "\n",
    "print(\"\\n4. 其他统计量:\")\n",
    "print(f\"标准差: {x.std()}\")                # 2.5819887\n",
    "print(f\"方差: {x.var()}\")                  # 6.6666665\n",
    "print(f\"中位数: {x.median()}\")             # 5\n",
    "print(f\"众数: {torch.mode(x.flatten())}\")  # 需要一维Tensor\n",
    "\n",
    "print(\"\\n5. 累加运算:\")\n",
    "print(f\"累积和: {x.cumsum(dim=0)}\")        # 按列累积\n",
    "print(f\"累积积: {x.cumprod(dim=1)}\")       # 按行累积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cad6be",
   "metadata": {},
   "source": [
    "8. GPU加速（CUDA）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5310d5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU加速演示:\n",
      "GPU可用: NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "CPU Tensor设备: cpu\n",
      "GPU Tensor设备: cuda:0\n",
      "直接创建在设备: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU加速演示:\")\n",
    "\n",
    "# 检查GPU是否可用\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU可用: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # 创建Tensor并移到GPU\n",
    "    cpu_tensor = torch.randn(1000, 1000)\n",
    "    gpu_tensor = cpu_tensor.cuda()  # 或 .to('cuda')\n",
    "    \n",
    "    print(f\"CPU Tensor设备: {cpu_tensor.device}\")\n",
    "    print(f\"GPU Tensor设备: {gpu_tensor.device}\")\n",
    "    \n",
    "    # 在GPU上进行计算\n",
    "    gpu_result = gpu_tensor @ gpu_tensor.T\n",
    "    \n",
    "    # 移回CPU\n",
    "    cpu_result = gpu_result.cpu()  # 或 .to('cpu')\n",
    "    \n",
    "    # 设备管理最佳实践\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    x = torch.randn(10, 10, device=device)  # 直接创建在指定设备\n",
    "    print(f\"直接创建在设备: {x.device}\")\n",
    "else:\n",
    "    print(\"GPU不可用，使用CPU计算\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd79a3",
   "metadata": {},
   "source": [
    "9. Tensor与NumPy互转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e443e033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor ↔ NumPy 无缝转换:\n",
      "Tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "转换为NumPy:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "修改后Tensor:\n",
      "tensor([[99,  2],\n",
      "        [ 3,  4]])\n",
      "\n",
      "NumPy数组:\n",
      "[[5 6]\n",
      " [7 8]]\n",
      "转换为Tensor:\n",
      "tensor([[5, 6],\n",
      "        [7, 8]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensor ↔ NumPy 无缝转换:\")\n",
    "\n",
    "# Tensor → NumPy\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "numpy_array = tensor.numpy()  # 共享内存！\n",
    "print(f\"Tensor:\\n{tensor}\")\n",
    "print(f\"转换为NumPy:\\n{numpy_array}\")\n",
    "\n",
    "# 修改NumPy会影响Tensor\n",
    "numpy_array[0, 0] = 99\n",
    "print(f\"修改后Tensor:\\n{tensor}\")  # 也会被修改！\n",
    "\n",
    "# NumPy → Tensor\n",
    "np_array = np.array([[5, 6], [7, 8]])\n",
    "tensor_from_np = torch.from_numpy(np_array)  # 同样共享内存\n",
    "print(f\"\\nNumPy数组:\\n{np_array}\")\n",
    "print(f\"转换为Tensor:\\n{tensor_from_np}\")\n",
    "\n",
    "# 不共享内存的转换（安全但耗内存）\n",
    "tensor_copy = torch.tensor(np_array)  # 创建副本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0c159d",
   "metadata": {},
   "source": [
    "10. Tensor与NumPy的性能对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a8a838a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "性能对比: NumPy vs PyTorch (CPU)\n",
      "数据大小: 10000×10000 = 100,000,000 个元素\n",
      "NumPy矩阵乘法: 8.888 秒\n",
      "PyTorch CPU乘法: 3.484 秒\n",
      "PyTorch GPU乘法: 0.585 秒\n",
      "GPU加速比: 6.0x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 创建大型数组\n",
    "size = 10000\n",
    "np_arr = np.random.randn(size, size)\n",
    "torch_tensor = torch.randn(size, size)\n",
    "\n",
    "print(\"性能对比: NumPy vs PyTorch (CPU)\")\n",
    "print(f\"数据大小: {size}×{size} = {size*size:,} 个元素\")\n",
    "\n",
    "# NumPy运算时间\n",
    "start = time.time()\n",
    "result_np = np_arr @ np_arr\n",
    "np_time = time.time() - start\n",
    "print(f\"NumPy矩阵乘法: {np_time:.3f} 秒\")\n",
    "\n",
    "# PyTorch CPU运算时间\n",
    "start = time.time()\n",
    "result_torch = torch_tensor @ torch_tensor\n",
    "torch_cpu_time = time.time() - start\n",
    "print(f\"PyTorch CPU乘法: {torch_cpu_time:.3f} 秒\")\n",
    "\n",
    "# PyTorch GPU运算时间（如果可用）\n",
    "if torch.cuda.is_available():\n",
    "    torch_gpu = torch_tensor.cuda()\n",
    "    torch.cuda.synchronize()  # 等待CUDA操作完成\n",
    "    start = time.time()\n",
    "    result_gpu = torch_gpu @ torch_gpu\n",
    "    torch.cuda.synchronize()\n",
    "    torch_gpu_time = time.time() - start\n",
    "    print(f\"PyTorch GPU乘法: {torch_gpu_time:.3f} 秒\")\n",
    "    print(f\"GPU加速比: {torch_cpu_time/torch_gpu_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1769a3d",
   "metadata": {},
   "source": [
    "11. 自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c85dfefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch梯度: tensor([2., 4., 6.])\n",
      "NumPy没有requires_grad和.grad属性\n"
     ]
    }
   ],
   "source": [
    "# PyTorch支持梯度跟踪\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "y.sum().backward()\n",
    "print(f\"PyTorch梯度: {x.grad}\")  # tensor([2., 4., 6.])\n",
    "\n",
    "# NumPy没有自动求导功能\n",
    "print(\"NumPy没有requires_grad和.grad属性\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
